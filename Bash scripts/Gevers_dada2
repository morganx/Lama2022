library(dada2); packageVersion("dada2")
# File parsing
#make files called fwd and rev first
pathF <- "/SCRATCH2/XCM_STUDENTS/Lama/RISK/stool/fwd/" 
#pathR <- "/SCRATCH2/XCM_STUDENTS/Lama/RISK/16S_rev" 
filtpathF <- file.path(pathF, "filtered") # Filtered forward files go into the pathF/filtered/ subdirectory
#filtpathR <- file.path(pathR, "filtered") # ...
fastqFs <- sort(list.files(pathF, pattern="fastq.gz"))
#fastqRs <- sort(list.files(pathR, pattern="fastq.gz"))
sample.names <- sapply(strsplit(fastqFs, "_1.fastq.gz"), `[`, 1)

#inspect quality before trimming
#pdf("Plotquality.pdf")
#plotQualityProfile(pathF[1])
#plotQualityProfile(pathR[1])
#dev.off() #Turn off saving as a pdf function

#sample.names<-sub("_L001$", "", sample.names, perl=TRUE)
if(length(fastqFs) != length(fastqRs)) stop("Forward and reverse files do not match.")
# Filtering: THESE PARAMETERS ARENT OPTIMAL FOR ALL DATASE
out<-filterAndTrim(fwd=file.path(pathF, fastqFs), filt=file.path(filtpathF, fastqFs),
              rev=file.path(pathR, fastqRs), filt.rev=file.path(filtpathR, fastqRs),
              truncLen=c(150,130), maxEE=2, truncQ=11, maxN=0, rm.phix=TRUE,
              compress=TRUE, verbose=TRUE, multithread=TRUE)

library(dada2); packageVersion("dada2")
# File parsing
filtpathF <- "/SCRATCH2/XCM_STUDENTS/Lama/RISK/16S_fwd/filtered" 
filtpathR <- "/SCRATCH2/XCM_STUDENTS/Lama/RISK/16S_rev/filtered"
#mykeep <- out[,"reads.out"] > 150 # Or other cutoff
filtFs <- list.files(filtpathF, pattern="fastq.gz", full.names = TRUE)
filtRs <- list.files(filtpathR, pattern="fastq.gz", full.names = TRUE)

sample.names <- sapply(strsplit(filtFs, "_1.fastq.gz"), `[`, 1)
sample.names1 <- sapply(strsplit(sample.names, "/"), `[`, 8)
names(filtFs) <- sample.names1
names(filtRs) <- sample.names1
set.seed(100)
# Learn forward error rates
errF <- learnErrors(filtFs, nbases=1e8, multithread=TRUE)
# Learn reverse error rates
errR <- learnErrors(filtRs, nbases=1e8, multithread=TRUE)
# Sample inference and merger of paired-end reads
mergers <- vector("list", length(sample.names1))
names(mergers) <- sample.names1
for(sam in sample.names1) {
  cat("Processing:", sam, "\n")
  derepF <- derepFastq(filtFs[[sam]])
  ddF <- dada(derepF, err=errF, multithread=TRUE)
  derepR <- derepFastq(filtRs[[sam]])
  ddR <- dada(derepR, err=errR, multithread=TRUE)
  merger <- mergePairs(ddF, derepF, ddR, derepR)
  mergers[[sam]] <- merger
}
rm(derepF); rm(derepR)
# Construct sequence table and remove chimeras
seqtab <- makeSequenceTable(mergers)
saveRDS(seqtab, "/SCRATCH2/XCM_STUDENTS/Lama/RISK/output/seqtab.rds") 
library(dada2); packageVersion("dada2")
# Merge multiple runs (if necessary)
st1 <- readRDS("/SCRATCH2/XCM_STUDENTS/Lama/RISK/output/seqtab.rds") 
#st2 <- readRDS("/SCRATCH/XCM_STUDENTS/output/seqtab.rds")
#st5 <- readRDS("/SCRATCH2/XCM_STUDENTS/Lama/RISK/output/seqtab.rds")
#st.all <- mergeSequenceTables(st1, st2, st3)
# Remove chimeras
seqtab <- removeBimeraDenovo(st1, method="consensus", multithread=TRUE)
# Assign taxonomy
# Do not change. database lives here
tax <- assignTaxonomy(seqtab, "/DATABASES/SILVA/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
# Write to disk
saveRDS(seqtab, "/SCRATCH2/XCM_STUDENTS/Lama/RISK/output/seqtab_final.rds") 
saveRDS(tax, "/SCRATCH2/XCM_STUDENTS/Lama/RISK/output/tax_final.rds")

